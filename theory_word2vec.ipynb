{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует множество инструментов, которые, обучившись на данных, умеют сопоставлять каждому слову языка некий числовой вектор, который описывает его синтаксические и семантические характеристики. Например, это может быть тематическое моделирование. Мы будем говорить о word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of embeddings based on factorizations: http://laxxx.org/seminar-20150608-handout.pdf\n",
    "\n",
    "Rather good overview of riemannian manifold mathematics: http://www.zfm.ethz.ch/~karrasch/Intro_Grassmann.pdf\n",
    "\n",
    "Major articles:\n",
    "1. [2013, Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf)<br>\n",
    "First basic article about word2vec\n",
    "2. [2013, Distributed representations of words and phrases and their compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)<br>\n",
    "Second basic article about word2vec\n",
    "3. [2014, word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method](http://arxiv.org/pdf/1402.3722.pdf)<br>\n",
    "Explanation, what is Skip-Gram model and what is Negative Sampling, which are used in word2vec\n",
    "4. [2014, Neural Word Embedding as Implicit Matrix Factorization](https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf)<br>\n",
    "Skip-gram with negative sampling in matrix factorization point of view. Interesting analytical insights, but not interesting experiments\n",
    "5. [2015, Improving Distributional Similarity with Lessons Learned from Word Embeddings](http://www.openu.ac.il/ISCOL2015/downloads/ISCOL2015_submission25_e_2.pdf)<br>\n",
    "Not very relevant now\n",
    "6. [2015, Word Embedding Revisited: A New Representation Learning and Explicit Matrix Factorization Perspective](http://home.ustc.edu.cn/~tianfei/Fei%20Tian's%20Homepage_files/papers/ijcai15.pdf)<br>\n",
    "Authors provide matrix factorization loss in case of decomposition of the co-occurence matrix. Also they provide simple alternating algorithm for optimization. Interesting, that it outperforms SGNS in case of noisy datasets (minimal word count is low). It is very promising — matrix factorizations give really better performance! However this form of loss function doesn't seem very useful for us — it is easier to approximate the mutual information matrix.\n",
    "7. [2015, A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/pdf/1510.00726.pdf)<br>\n",
    "Overview of neural nets in NLP\n",
    "8. [2015, Evaluation methods for unsupervised word embeddings](https://www.cs.cmu.edu/~ark/EMNLP-2015/proceedings/EMNLP/pdf/EMNLP036.pdf)<br>\n",
    "Authors analyse different methods for evaluation of word embeddings. Not very comprehensive overview :(\n",
    "9. [2015, Model-based Word Embeddings from Decompositions of Count Matrices](http://www1.cs.columbia.edu/~stratos/research/acl15cca.pdf)<br>\n",
    "Authors try to decompose different matrices, which elements depend on word counters (not only shifted mutual information). Results are pretty comparable to word2vec.\n",
    "10. [2015, How to Generate a Good Word Embedding?](http://arxiv.org/pdf/1507.05523.pdf)<br>\n",
    "Rather good explanation, comparison and insights about different embedding methods\n",
    "11. [2015, Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds](http://arxiv.org/pdf/1507.07636.pdf)<br>\n",
    "Authors noticed that cosine distance, which is usually used to compare word embeddings, is ad-hod doesn't have to be optimal in terms of analogy tasks. Using riemannian geometry of word points in the embedding space, the authors propose method to build more accurate distance between words.\n",
    "12. [2015, OUTPERFORMING WORD2VEC ON ANALOGY TASKS WITH RANDOM PROJECTIONS](http://arxiv.org/pdf/1412.6616v1.pdf)<br>\n",
    "Interesting ad-hoc approach to train model of embeddings (without gradient descent, matrix fatorizations, etc)\n",
    "13. [2015, A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution](http://arxiv.org/pdf/1508.03826.pdf)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как описывается в последних статьях, word2vec можно рассматривать как матричное разложение. В связи с этим есть идея — давайте попробуем использовать римановскую оптимизацию для обучения word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People mean different models when talk about word2vec. We are talking about Skip-Gram with Negative Sampling model (SGNS) [1,2,3]. The result of this model is a low dimensional vector for each word that describes its syntactic and semantic properties. \n",
    "\n",
    "Training of this model is equal [6] to following matrix factorization problem:\n",
    "$$D \\approx C^TW,\\quad D\\in\\mathbb{R}^{|V_c|\\times |V_w|}, \\quad C \\in\\mathbb{R}^{d\\times |V_c|}, \\quad W \\in\\mathbb{R}^{d\\times |V_w|},$$\n",
    "where $D$ is a training corpus matrix, $C$ is a context matrix and $W$ is a word matrix.\n",
    "\n",
    "Let us define the following values:\n",
    "$$\n",
    "\\#(w,c) = D_{wc}, \\; \\#(w) = \\sum_{c \\in V_c} \\#(w,c), \\; \\#(c) = \\sum_{w \\in V_w} \\#(w,c), \\; |\\mathcal{D}| = \\sum_{w \\in V_w} \\sum_{c \\in V_c} \\#(w,c)\n",
    "$$\n",
    "\n",
    "Then the loss function is computed in the following way (see Equation (5) in [4] for details):\n",
    "$$l(w,c) = \\#(w,c)\\log\\sigma(\\vec{w}\\cdot\\vec{c})+k\\frac{\\#(w)\\#(c)}{|\\mathcal{D}|}\\log\\sigma(-\\vec{w}\\cdot\\vec{c}),$$\n",
    "where $\\vec{c}$ and $\\vec{w}$ are the columns of matrices $C$ and $W$ respectively. \n",
    "\n",
    "Authors in [4] used MSE measure and SVD algorithm for this low-rank problem, instead of $l(w,c)$ optimization. We can use low-rank optimization methods to search better solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/matrices.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement as explicit matrix factorization (EMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Matrix to decompose:**\n",
    "$$\n",
    "\\mathbf{D} \\in \\mathbb{R}^{|V_c| \\times |V_w|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Factor matrices:**\n",
    "$$\n",
    "\\mathbf{D} \\approx \\mathbf{C^TW}, \\; \\mathbf{C} \\in \\mathbb{R}^{|V_c| \\times d},\\; \\mathbf{W} \\in \\mathbb{R}^{d \\times |V_w|}\n",
    "$$\n",
    "$\\mathbf{w}$ and $\\mathbf{c}$ are the columns of matrices $\\mathbf{W}$ and $\\mathbf{C}$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Some supporting values**\n",
    "$$\n",
    "\\#(w,c) = D_{wc}, \\\\\n",
    "\\#(w) = \\sum_{c \\in V_c} \\#(w,c), \\\\\n",
    "\\#(c) = \\sum_{w \\in V_w} \\#(w,c), \\\\\n",
    "|\\mathcal{D}| = \\sum_{w \\in V_w} \\sum_{c \\in V_c} \\#(w,c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Functional:**\n",
    "$$\n",
    "\\textbf{MF}(\\mathbf{D},\\mathbf{C^TW}) = \\sum_{w \\in V_w} \\sum_{c \\in V_c} \\left[ \\#(w,c) \\log\\sigma(\\mathbf{c^Tw}) + k\\frac{\\#(w)\\#(c)}{|\\mathcal{D}|} \\log\\sigma(-\\mathbf{c^Tw}) \\right] \\rightarrow \\min_{C,W} \\\\\n",
    "\\textbf{MF}(\\mathbf{D},\\mathbf{C^TW}) = \\sum_{w \\in V_w} \\sum_{c \\in V_c} \\left[ d_{wc} \\log\\sigma(\\mathbf{c^Tw}) + b_{wc} \\log\\sigma(-\\mathbf{c^Tw}) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Gradient:**\n",
    "$$\n",
    "\\nabla\\mathbf{MF(C,W)}_{cw} = \\left[\\frac{\\partial\\textbf{MF}(\\mathbf{D},\\mathbf{C^TW})}{\\partial \\mathbf{C^TW}}\\right]_{cw} = \\#(w,c)\\sigma(\\mathbf{-c^Tw}) - k\\frac{\\#(w)\\#(c)}{|\\mathcal{D}|} \\sigma(\\mathbf{c^Tw}) \\\\\n",
    "\\nabla\\mathbf{MF(C,W)}_{cw} = d_{wc} \\sigma(\\mathbf{-c^Tw}) - b_{wc} \\sigma(\\mathbf{c^Tw})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here $d_{wc}$ and $b_{wc}$ does not depend on factors, they are only depend on matrix $\\mathbf{D}$ and negative sampling parameter $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for solving EMF problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Alternating minimization for EMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternating minimization is a simple iterative optimization procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 1. Alternating minimization for explicit matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input:** Co-occurrence matrix $\\mathbf{D}$, step-size of gradient descent $\\eta$, maximum number of iterations $K$\n",
    "\n",
    "**Output:** $\\mathbf{C}_K$, $\\mathbf{W}_K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Initialize $\\mathbf{C}_i$ and $\\mathbf{W}_i$ randomly, $i=1$.\n",
    "\n",
    "**2.** $\\textbf{while}$ $i \\leq K$ $\\textbf{do}$\n",
    "\n",
    "**3.** $\\;\\mathbf{W}_i = \\mathbf{W}_{i-1}$\n",
    "\n",
    "$\\;\\;\\;\\; \\textbf{repeat}$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\mathbf{W}_i=\\mathbf{W}_i - \\mathbf{C}_{i-1} \\times \\nabla\\textbf{MF} {\\mathbf{(C_{i-1},W_i)}}$\n",
    "\n",
    "$\\;\\;\\;\\; \\textbf{until}$ $\\textit{Convergence}$\n",
    "\n",
    "**4.** $\\;\\mathbf{C}_i = \\mathbf{C}_{i-1}$\n",
    "\n",
    "$\\;\\;\\;\\; \\textbf{repeat}$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\mathbf{C}_i=\\mathbf{C}_i - \\mathbf{W}_{i} \\times \\nabla\\textbf{MF} {\\mathbf{(C_{i},W_i)}}$\n",
    "\n",
    "$\\;\\;\\;\\; \\textbf{until}$ $\\textit{Convergence}$\n",
    "\n",
    "**5.** $i = i + 1$\n",
    "\n",
    "**6.** $\\textbf{end while}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this algorithm we are minimizing $\\mathbf{MF(D,C^TW)}$, so by $\\textit{Convergence}$ we mean $\\|\\mathbf{MF_{old}(D,C^TW)} - \\mathbf{MF_{new}(D,C^TW)}\\|<\\textit{tol}$, where indices **old** and **new** correspond to the previous and the current iteration inside **repeat** respectively and $\\textit{tol}$ is some tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Riemannian optimization on low-rank manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal{M}_d$ be a manifold of $d$-rank matrices in $\\mathbb{R}^{|V_c|\\times|V_w|}$, $\\mathcal{M}_d = \\{X \\in \\mathbb{R}^{|V_c|\\times|V_w|}: \\text{rank}\\;X=d\\}$ and $T_X\\mathcal{M}_d$ be a tangent space to this manifold in a point $X$. We want to minimize our functional $\\mathbf{MF}(X)$ in $\\mathcal{M}_d$ (to find the point $X^*$ such as $X^* = \\arg\\min_{X \\in \\mathcal{M}_d} \\mathbf{MF}(X)$). This problem is hard as constrained optimization problem, because it is hard to write down $X \\in \\mathcal{M}_d$ as explicit expression. To solve this problem, we use the following two-step gradient descent procedure.\n",
    "\n",
    "Let $X \\in \\mathcal{M}_d$ be a point from our manifold, $X=U \\Sigma V^T$ is its SVD and $\\nabla\\mathbf{MF}(X)$ is a gradient of our functional in the point $X$. Further for notational simplicity we consider $\\nabla\\mathbf{MF}(X)=F$.\n",
    "\n",
    "**Step 1 (Projection).** As a gradient vector of our functional $\\nabla\\mathbf{MF}(X)=F$ only gives a direction in the whole space $\\mathbb{R}^{|V_c|\\times|V_w|}$, at the first step we project it onto the tangent space $T_X\\mathcal{M}_d$.  Defining $P_U=UU^T$, $P_U^{\\bot} = I - UU^T$, $P_V = VV^T$ and $P_V^{\\bot} = I - VV^T$ we denote the orthogonal projection onto the tangent space at $X$ as\n",
    "\n",
    "$$\n",
    "P_{T_X\\mathcal{M}_d}: \\mathbb{R}^{|V_c|\\times|V_w|} \\rightarrow T_X\\mathcal{M}_d, \\\\\n",
    "F \\rightarrow \\Pi_F = P_U F P_V + P_U^{\\bot} F P_V + P_U F P_V^{\\bot}.\n",
    "$$\n",
    "\n",
    "**Step 2 (Retraction).** As a tangent vector only gives a direction but not the line search itself on the manifold, at the second step a smooth mapping $R_X: T_X\\mathcal{M}_d \\rightarrow \\mathcal{M}_d$ called the retraction is needed to map tangent vectors to the manifold.\n",
    "\n",
    "$$\n",
    "R_X: T_X\\mathcal{M}_d \\rightarrow \\mathcal{M}_d, \\\\\n",
    "\\Pi_F \\rightarrow M_F, \\\\\n",
    "M_F = \\arg\\min_{Z \\in \\mathcal{M}_d} \\|X + \\Pi_F - Z\\|_F.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization algorithm based on proposed method will look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 2. Riemannian optimization for explicit matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Initialize $X_0 \\in \\mathcal{M}_d,\\; i=1$\n",
    "\n",
    "**2.** $\\textbf{while}$ $i \\leq K$ $\\textbf{do}$\n",
    "\n",
    "**3.** $\\;\\;F_i = \\nabla\\textbf{MF}(X_{i-1})$\n",
    "\n",
    "**4.** $\\;\\;\\Pi_i = P_{T_{X_i}\\mathcal{M}_d}(F_i)$\n",
    "\n",
    "**5.** $\\;\\;M_i = R_{X_i}(\\Pi_i)$ \n",
    "\n",
    "**6.** $\\;\\;X_i = X_{i-1} + \\eta M_i$ \n",
    "\n",
    "**7.** $\\textbf{end while}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Projector splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 3. Projector splitting for explicit matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Initialize $X_0 = C_0^T W_0\\in \\mathcal{M}_d,\\; i=1$\n",
    "\n",
    "**2.** $\\textbf{while}$ $i \\leq K$ $\\textbf{do}$\n",
    "\n",
    "**3.** $\\;\\;U_{i-1}, S_{i-1}, V_{i-1}^T = \\text{SVD}(X_{i-1})$\n",
    "\n",
    "**4.** $\\;\\; C_{i-1} = U_{i-1}\\sqrt{S_{i-1}},\\;\\;W_{i-1} = \\sqrt{S_{i-1}}{V_{i-1}}^T$\n",
    "\n",
    "**5.** $\\;\\;F_i = \\nabla\\textbf{MF}(X_{i-1})$\n",
    "\n",
    "**6.** $\\;\\;U_i, \\_ = \\text{QR}((X_{i-1}+F_i)V_{i-1})$\n",
    "\n",
    "**7.** $\\;\\;V_i^T, S_{i}^T = \\text{QR}((X_{i-1}+F_i)^TU_i)$\n",
    "\n",
    "**8.** $\\;\\;X_i = U_i S_i V_i^T$\n",
    "\n",
    "**9.** $\\textbf{end while}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stochastic PS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "* ss -- sample_size (how many words we sample)\n",
    "* ws -- window_size (size of the window around the word, all words within this window are considered contexts), **default=4**\n",
    "* k -- negative sampling parameter **default=5**\n",
    "\n",
    "#### One iteration of algorithm\n",
    "**1.** Sample $ss$ words $W_s = \\{w_1,w_2,\\dots,w_{ss}\\}$ from the distribution $p(w)$\n",
    "\n",
    "**2.** For each word $w \\in W_s$\n",
    "\n",
    "$\\;\\;$ **2a.** Sample $ws$ contexts $C_{pos}=\\{c_1,c_2,\\dots,c_{ws}\\}$ from the distribution $p(c|w)$. For each $c \\in C_{pos}$ change the corresponding value of $\\nabla\\textbf{MF}_{cw}$ by its contribution (encouraging observed pairs): \n",
    "$$\\nabla\\textbf{MF}_{cw} = \\nabla\\textbf{MF}_{cw} + \\sigma(\\mathbf{-c^Tw})$$\n",
    "\n",
    "$\\;\\;$ **2b.** Sample $k$ contexts $C_{neg}=\\{c_1,c_2,\\dots,c_{k}\\}$ from the unigram distribution on $c$. For each $c_N \\in C_{neg}$ change the corresponding value of $\\nabla\\textbf{MF}_{cw}$ by its contribution (penalizing random pairs):\n",
    "$$\\nabla\\textbf{MF}_{cw} = \\nabla\\textbf{MF}_{cw} - \\frac{1}{k} \\sum_{c_N \\in C_{neg}}\\sigma(\\mathbf{c_N^Tw})$$\n",
    "\n",
    "**3.** Move our solution in a direction of obtained stochastic gradient $\\nabla\\textbf{MF}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic tasks and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarity\n",
    "\n",
    "In word similarity tasks we estimate the measure of closeness of two different words. Datasets for this task contain pairs of words and assesor's score given to them (the more the score is the more similar the words are).\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "1) Original wordsim353. 353 pairs of words rated from 0 to 10. Each pair was rated by 13 or 16 assesors and the mean grade was taken. (http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)\n",
    "\n",
    "2) Similarity wordsim353. 203 pairs of words, subset of original wordsim353 which aim to measure similarity of two words.\n",
    "\n",
    "3) Relatedness wordsim353. 252 pairs of words, subset of original wordsim353 which aim to measure relatedness of two words.\n",
    "\n",
    "4) MEN. 3000 pairs of words rated from 0 to 50. Each pair was rated by 50 assesors and the sum grade was taken. (http://clic.cimec.unitn.it/~elia.bruni/MEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analogical reasoning\n",
    "\n",
    "In analogical reasoning tasks we want to predict missing word from the statement which encounter analogy (e.g. in phrase \"London for England is Paris for \\_\\_\\_\\_\\_\" the missing word is most likely \"France\").\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "1) Google Query dataset for analogcial reasoning contains 19544 fourtuples of words, the task is to predict the forth one given the first three words. This dataset cover 14 different analogy relations (e.g. country-capital, country-currency, nationality-adjective etc)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
